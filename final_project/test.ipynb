{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "from sklearn.preprocessing import Imputer\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import tester\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Needed for Features selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### Needed for Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "### Needed for tuning the model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define the various feature lists needed\n",
    "\n",
    "# all the intital featurs in dataset except for email_address and shared_receipt_with_poi\n",
    "initial_features_list = ['poi',\n",
    "                'salary',\n",
    "                'bonus', \n",
    "                'long_term_incentive', \n",
    "                'deferred_income', \n",
    "                'deferral_payments',\n",
    "                'loan_advances', \n",
    "                'other',\n",
    "                'expenses', \n",
    "                'director_fees',\n",
    "                'total_payments',\n",
    "                'exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'total_stock_value',\n",
    "                'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']\n",
    "\n",
    "payment_feature_list = ['salary',\n",
    "            'bonus', \n",
    "            'long_term_incentive', \n",
    "            'deferred_income', \n",
    "            'deferral_payments',\n",
    "            'loan_advances', \n",
    "            'other',\n",
    "            'expenses', \n",
    "            'director_fees']\n",
    "\n",
    "stock_feature_list = ['exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred']\n",
    "\n",
    "email_feature_list = [  'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']\n",
    "\n",
    "features_list = ['poi', 'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in the dataset: 146\n",
      "Number of features per person in the dataset: 21\n",
      "features included in the dataset: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['salary',\n",
       " 'to_messages',\n",
       " 'deferral_payments',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options',\n",
       " 'bonus',\n",
       " 'restricted_stock',\n",
       " 'shared_receipt_with_poi',\n",
       " 'restricted_stock_deferred',\n",
       " 'total_stock_value',\n",
       " 'expenses',\n",
       " 'loan_advances',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'from_this_person_to_poi',\n",
       " 'poi',\n",
       " 'director_fees',\n",
       " 'deferred_income',\n",
       " 'long_term_incentive',\n",
       " 'email_address',\n",
       " 'from_poi_to_this_person']"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Exploring the dataset: 146 people, 21 features\n",
    "print \"Number of people in the dataset: {0}\".format(len(my_dataset))\n",
    "print \"Number of features per person in the dataset: {0}\".format(len(my_dataset.values()[0]))\n",
    "print \"features included in the dataset: \" \n",
    "my_dataset.values()[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total NaN values in the dataset: 1263\n",
      "total values in the datase: 25508\n",
      " total number of rows with NaN values: 146\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 19 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "dtypes: bool(1), float64(18)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "### Transform data from dictionary to the Pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(data_dict, orient = 'index')\n",
    "\n",
    "#Order columns in DataFrame, excluding email\n",
    "df = df[initial_features_list]\n",
    "df = df.replace('NaN', np.nan)\n",
    "\n",
    "print \"total NaN values in the dataset: {0}\" .format (df.isnull().sum().sum())\n",
    "print \"total values in the datase: {0}\" .format (sum (df.count () + df.isnull().sum().sum()))\n",
    "print \" total number of rows with NaN values: {0}\"\\\n",
    ".format (sum([True for idx,row in df.iterrows() if any(row.isnull())]))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total NaN values in the dataset is 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 19 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       146 non-null float64\n",
      "bonus                        146 non-null float64\n",
      "long_term_incentive          146 non-null float64\n",
      "deferred_income              146 non-null float64\n",
      "deferral_payments            146 non-null float64\n",
      "loan_advances                146 non-null float64\n",
      "other                        146 non-null float64\n",
      "expenses                     146 non-null float64\n",
      "director_fees                146 non-null float64\n",
      "total_payments               146 non-null float64\n",
      "exercised_stock_options      146 non-null float64\n",
      "restricted_stock             146 non-null float64\n",
      "restricted_stock_deferred    146 non-null float64\n",
      "total_stock_value            146 non-null float64\n",
      "to_messages                  146 non-null float64\n",
      "from_messages                146 non-null float64\n",
      "from_this_person_to_poi      146 non-null float64\n",
      "from_poi_to_this_person      146 non-null float64\n",
      "dtypes: bool(1), float64(18)\n",
      "memory usage: 26.8+ KB\n"
     ]
    }
   ],
   "source": [
    "### impute the \"NaN\" values\n",
    "\n",
    "# Replace \"NaN\" values in financial dataset with 0\n",
    "df.iloc [:,:15] = df.iloc [:,:15].fillna(0)\n",
    "\n",
    "# Replace \"NaN\" values in email feautre set with median values for two categories POI = 1 and POI = 0\n",
    "email_features = ['to_messages', 'from_messages', 'from_this_person_to_poi', 'from_poi_to_this_person']\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "df.loc[df[df.poi == 1].index,email_features] = imp.fit_transform(df[email_features][df.poi == 1])\n",
    "df.loc[df[df.poi == 0].index,email_features] = imp.fit_transform(df[email_features][df.poi == 0])\n",
    "\n",
    "# Reviewing to see if we still have NaN values\n",
    "print \"total NaN values in the dataset is {0}\" .format (df.isnull().sum().sum())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [poi, salary, bonus, long_term_incentive, deferred_income, deferral_payments, loan_advances, other, expenses, director_fees, total_payments, exercised_stock_options, restricted_stock, restricted_stock_deferred, total_stock_value, to_messages, from_messages, from_this_person_to_poi, from_poi_to_this_person]\n",
       "Index: []"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 2: Data cleansing: check for typos and miscalculations\n",
    "\n",
    "# Task 2.1: check for the accuracy of money payments data: summing payments features and comparing with total_payments\n",
    "df[df[payment_feature_list].sum(axis='columns') != df.total_payments]\n",
    "\n",
    "# Task 2.2: check for the accuracy of stock payments: summing stock payments and comparing with total_stock_value\n",
    "df[df[stock_feature_list].sum(axis='columns') != df.total_stock_value]\n",
    "\n",
    "# Task 2.3: Correct the errors for total_payments and total_stock_value based on the data in the PDF file\n",
    "df.loc['BELFER ROBERT','total_payments'] = 3285\n",
    "df.loc['BELFER ROBERT','deferral_payments'] = 0\n",
    "df.loc['BELFER ROBERT','restricted_stock'] = 44093\n",
    "df.loc['BELFER ROBERT','restricted_stock_deferred'] = -44093\n",
    "df.loc['BELFER ROBERT','total_stock_value'] = 0\n",
    "df.loc['BELFER ROBERT','director_fees'] = 102500\n",
    "df.loc['BELFER ROBERT','deferred_income'] = -102500\n",
    "df.loc['BELFER ROBERT','exercised_stock_options'] = 0\n",
    "df.loc['BELFER ROBERT','expenses'] = 3285\n",
    "df.loc['BELFER ROBERT',]\n",
    "df.loc['BHATNAGAR SANJAY','expenses'] = 137864\n",
    "df.loc['BHATNAGAR SANJAY','total_payments'] = 137864\n",
    "df.loc['BHATNAGAR SANJAY','exercised_stock_options'] = 1.54563e+07\n",
    "df.loc['BHATNAGAR SANJAY','restricted_stock'] = 2.60449e+06\n",
    "df.loc['BHATNAGAR SANJAY','restricted_stock_deferred'] = -2.60449e+06\n",
    "df.loc['BHATNAGAR SANJAY','other'] = 0\n",
    "df.loc['BHATNAGAR SANJAY','director_fees'] = 0\n",
    "df.loc['BHATNAGAR SANJAY','total_stock_value'] = 1.54563e+07\n",
    "df.loc['BHATNAGAR SANJAY',]\n",
    "\n",
    "# Reviewing to see if the totals are correct now\n",
    "df[df[payment_feature_list].sum(axis='columns') != df.total_payments]\n",
    "df[df[stock_feature_list].sum(axis='columns') != df.total_stock_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.4: identify & remove the outliers using interquantile range (IQR) in descriptive statistics\n",
    "# IQR = df.quantile(.75)-df.quantile(.25)\n",
    "# Upper outliers definition: df.quantile(.75) + (1.5 * IQR)\n",
    "# lower outliers definition: df.quantile(.25) - (1.5 * IQR)\n",
    "\n",
    "# determine the number of lower outliers for each row/person => we will ignore this based on the results   \n",
    "lower_outliers = df.quantile(.25) - 1.5 * (df.quantile(.75)-df.quantile(.25))\n",
    "pd.DataFrame((df[1:] < lower_outliers[1:]).sum(axis = 1), columns = ['# of lower outliers']).\\\n",
    "    sort_values('# of lower outliers',  ascending = [0]).head(7)\n",
    "\n",
    "# determine the number of upper outliers for each row/person \n",
    "upper_outliers = df.quantile(.5) + 1.5 * (df.quantile(.75)-df.quantile(.25))\n",
    "pd.DataFrame((df[1:] > upper_outliers[1:]).sum(axis = 1), columns = ['# of upper outliers']).\\\n",
    "    sort_values('# of upper outliers',  ascending = [0]).head(7)\n",
    "\n",
    "# \"TOTAL\" doesn't add much value to the set so we will remove it.\n",
    "# Kenneth Lay and Jeffrey Skilling are very important personas in Enron case \n",
    "# We will leave the rest of the outliers since they maybe anomalies vs outliers\n",
    "df = df.drop(['TOTAL'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s) & store in the dataframe\n",
    "\n",
    "# feature scaling:fraction of person's email to POI to all sent messages\n",
    "df['to_poi_message_ratio'] = df['from_this_person_to_poi']/df['from_messages']\n",
    "#clean all 'inf' values which we got if the person's from_messages = 0\n",
    "df = df.replace('inf', 0)\n",
    "\n",
    "#feature scaling: fraction of person's email from POI to all messages received\n",
    "df['from_poi_message_ratio'] = df['from_poi_to_this_person']/df['to_messages']\n",
    "#clean all 'inf' values which we got if the person's to_messages = 0\n",
    "df = df.replace('inf', 0)\n",
    "\n",
    "initial_features_list.extend(['to_poi_message_ratio', 'from_poi_message_ratio'])\n",
    "features_list.extend(['to_poi_message_ratio', 'from_poi_message_ratio'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### normalize the training data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_norm = df[initial_features_list]\n",
    "df_norm = scaler.fit_transform(df_norm.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.86020\tPrecision: 0.47162\tRecall: 0.40300\tF1: 0.43462\tF2: 0.41508\n",
      "\tTotal predictions: 15000\tTrue positives:  806\tFalse positives:  903\tFalse negatives: 1194\tTrue negatives: 12097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Task 4.1: Trying GaussianNB\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Trying GaussianNB => eliminated for final model based on results\n",
    "clf = GaussianNB()\n",
    "temp_features_list = ['poi']+range(7)\n",
    "\n",
    "my_dataset_GNB = pd.DataFrame(SelectKBest(f_classif, k = 7).fit_transform(df_norm, df.poi), index = df.index)\n",
    "my_dataset_GNB.insert(0, \"poi\", df.poi)\n",
    "my_dataset_GNB = my_dataset_GNB.to_dict(orient = 'index')\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset_GNB, temp_features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.87213\tPrecision: 0.52774\tRecall: 0.39000\tF1: 0.44853\tF2: 0.41148\n",
      "\tTotal predictions: 15000\tTrue positives:  780\tFalse positives:  698\tFalse negatives: 1220\tTrue negatives: 12302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trying PCA + GaussianNB => eliminated for final model based on results\n",
    "pca = PCA(n_components=3)\n",
    "temp_features_list = ['poi']+range(3)\n",
    "my_dataset_GNB = pd.DataFrame(SelectKBest(f_classif, k=8).fit_transform(df_norm, df.poi), index = df.index)\n",
    "PCA_dataset = pd.DataFrame(pca.fit_transform(my_dataset_GNB),  index=df.index)\n",
    "PCA_dataset.insert(0, \"poi\", df.poi)\n",
    "PCA_dataset = PCA_dataset.to_dict(orient = 'index')  \n",
    "\n",
    "dump_classifier_and_data(clf, PCA_dataset, temp_features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=75, splitter='best')\n",
      "\tAccuracy: 0.86793\tPrecision: 0.50475\tRecall: 0.50500\tF1: 0.50487\tF2: 0.50495\n",
      "\tTotal predictions: 15000\tTrue positives: 1010\tFalse positives:  991\tFalse negatives:  990\tTrue negatives: 12009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trying Decision tree => eliminated for final model based on the results\n",
    "clf = DecisionTreeClassifier(random_state = 75)\n",
    "my_dataset_DT = df[initial_features_list].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset_DT, initial_features_list)\n",
    "tester.main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to_poi_message_ratio', 0.34758155230596183]\n",
      "['expenses', 0.31100626310600066]\n",
      "['to_messages', 0.13531641878098569]\n",
      "['total_stock_value', 0.084572761738116065]\n",
      "['deferred_income', 0.070477301448430049]\n",
      "['from_messages', 0.051045702620505777]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=75, splitter='best')\n",
      "\tAccuracy: 0.89500\tPrecision: 0.61530\tRecall: 0.56700\tF1: 0.59016\tF2: 0.57604\n",
      "\tTotal predictions: 15000\tTrue positives: 1134\tFalse positives:  709\tFalse negatives:  866\tTrue negatives: 12291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trying Decision tree with feature importance => eliminated for final model based on the results\n",
    "clf = DecisionTreeClassifier(random_state = 75)\n",
    "my_dataset_DT = df[initial_features_list].to_dict(orient = 'index')\n",
    "clf.fit(df_norm, df['poi'])\n",
    "\n",
    "# create and sort features_list of non-null importance features for the model\n",
    "features_importance = []\n",
    "len (clf.feature_importances_)\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "   if clf.feature_importances_[i] > 0:\n",
    "       features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for f_i in features_importance:\n",
    "    print f_i\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')\n",
    "\n",
    "# Searchgrid for tuning parameters\n",
    "param_grid = {'bootstrap': [False],\n",
    " 'criterion': ['entropy'],\n",
    " 'max_depth': [None],\n",
    " 'max_features': [1],\n",
    " 'min_samples_leaf': [1],\n",
    " 'min_samples_split': [9]}\n",
    "\n",
    "grid_search = GridSearchCV(clf,param_grid=param_grid)\n",
    "\n",
    "my_dataset_DT = df[features_list].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset_DT, features_list)\n",
    "tester.main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.88893\tPrecision: 0.66277\tRecall: 0.34000\tF1: 0.44944\tF2: 0.37669\n",
      "\tTotal predictions: 15000\tTrue positives:  680\tFalse positives:  346\tFalse negatives: 1320\tTrue negatives: 12654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. \n",
    "\n",
    "#data = featureFormat(my_dataset, initial_features_list, sort_keys = True)\n",
    "#labels, features = targetFeatureSplit(data)\n",
    "#features_train, features_test, labels_train, labels_test = \\\n",
    "#train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Trying random forest along with grid_search for tuning the parameters\n",
    "clf = RandomForestClassifier()\n",
    "my_dataset = df[features_list].to_dict(orient = 'index')\n",
    "\n",
    "# Searchgrid for random forest: specify parameters and distributions to sample from\n",
    "param_grid = {'bootstrap': [False],\n",
    " 'criterion': ['entropy'],\n",
    " 'max_depth': [None],\n",
    " 'max_features': [1],\n",
    " 'min_samples_leaf': [1],\n",
    " 'min_samples_split': [9]}\n",
    "\n",
    "grid_search = GridSearchCV(clf,param_grid=param_grid)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()\n",
    "#grid_search.fit(features_train, labels_train)\n",
    "#predictions=grid_search.predict(features_test)\n",
    "#print classification_report(labels_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
